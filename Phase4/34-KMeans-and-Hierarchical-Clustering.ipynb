{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $k$-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro\n",
    "\n",
    "#### Import libraries and download dataset\n",
    "\n",
    "The specific documentation for k-means can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages for today\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Need scipy for hierarchical clustering visualizations\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Familiar packages for plotting, data manipulation, and numeric functions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import a colleague's code for the demo clusters/visualizations\n",
    "import src.demo_images as demo\n",
    "import src.k_means_plotter as kmplot\n",
    "import src.hier_example as hier\n",
    "\n",
    "# Have plots appear in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Default plot params\n",
    "plt.style.use('seaborn')\n",
    "cmap = 'tab10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering!   Finding **GROUPS**\n",
    "\n",
    "How many groups do you see?\n",
    "\n",
    "<img src=\"images/initialscenario.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wait - How is clustering different from classification?\n",
    "\n",
    ">In _classification_ you **know** what groups are in the dataset and the goal is to _**predict**_ class membership accurately.\n",
    ">\n",
    ">In _clustering_ you **do not** know which groups are in the dataset and you are trying to _**identify**_ the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So what do you do with clustering results?\n",
    "\n",
    "Clustering is often an *informing* step in your analysis. Once clusters are identified, one can:\n",
    "- Create strategies on how to approach each group differently\n",
    "- Use cluster membership as an independent variable in a predictive model\n",
    "- Use the clusters as the _**target label**_ in future classification models. How would you assign new data to the existing clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Explore the algorithm with an intuitive K means approach\n",
    "\n",
    "### Observe the following four methods with a sample dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Method 1 | Method 2 |\n",
    "| -------- | -------- |\n",
    "| <img src=\"images/from-left.gif\" width=400> | <img src=\"images/from-right.gif\" width=400> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Method 3 | Method 4 |\n",
    "| -------- | -------- |\n",
    "| <img src=\"images/from-top.gif\" width=400> | <img src=\"images/from-bottom.gif\" width=400> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method Questions:\n",
    "\n",
    "- What do they have in common?\n",
    "- What are the differences between them?\n",
    "- How many groups are there in the end?\n",
    "- Do you see any problems with this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-means algorithm, at its core, in an optimization function\n",
    "\n",
    "<img src=\"images/minmaxdata.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reassigns groups and adjusts centroids to...\n",
    "\n",
    "<img src=\"images/min.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### And to...\n",
    "\n",
    "<img src=\"images/max.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Sci-kit Learn** documentation actually has some pretty good [documentation describing the algorithm](https://scikit-learn.org/stable/modules/clustering.html#k-mean) if you want more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Means Plotter - Introducing the Challenges of Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = datasets.make_blobs(centers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df = kmplot.k_means(X[:, 0], X[:, 1], k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "df = kmplot.k_means(X[:, 0], X[:, 1], k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "df = kmplot.k_means(X[:, 0], X[:, 1], k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df = kmplot.k_means(X[:, 0], X[:, 1], k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Assumptions** and **challenges** of $k$-means\n",
    "\n",
    "- Demonstrate the ideal $k$-means dataset\n",
    "- Show three scenarios where $k$-means struggles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideal $k$-means scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "demo.ideal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Meets all assumptions:\n",
    "\n",
    "- Independent variables\n",
    "- Balanced cluster sizes\n",
    "- Clusters have similar density\n",
    "- Spherical clusters/equal variance of variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem Scenario 1 - classes not all round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.messyOne()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem Scenario 2 - imbalanced class size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.messyTwo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem Scenario 3 - class size and density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.messyThree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solution to challenges:\n",
    "\n",
    "- Preprocessing: PCA or scaling\n",
    "- Try a different clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise:\n",
    "### $k$-means on larger dataset - Wine subscription\n",
    "\n",
    "You want to run a wine subscription service, but you have no idea about wine tasting notes. You are a person of science.\n",
    "You have a wine dataset of scientific measurements.\n",
    "If you know a customer likes a certain wine in the dataset, can you recommend other wines to the customer in the same cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Questions:\n",
    "- How many clusters are in the wine dataset?\n",
    "- What are the characteristics of each clusters?\n",
    "- What problems do you see potentially in the data?\n",
    "\n",
    "The dataset is `Wine.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Work on problem here: Would scaling make a difference?\n",
    "wine = pd.read_csv('data/Wine.csv')\n",
    "wine.drop(columns=['Customer_Segment'], inplace=True) # removing spoilers\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scale the data first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now - initialize KMeans! How many clusters...?\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put our labels back into our data\n",
    "labeled_df = pd.concat([wine, pd.DataFrame(model.labels_,\n",
    "                        columns=['cluster'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do these look like?\n",
    "labeled_df.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note! \n",
    "\n",
    "You may have different cluster centers - the algorithm is sensitive to starting points.\n",
    "\n",
    "Even if we set `n_init` to a significant value, it's still a good idea to use `random_state` to ensure repeatable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing the appropriate number for $k$\n",
    "\n",
    "#### Two metrics we can use: **elbow method** and the **silhouette coefficient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elbow Method\n",
    "\n",
    "Elbow method uses the sum of squared error (SSE) calculated from each instance of $k$ to find the best value of $k$.\n",
    "\n",
    "This is sometimes called the \"inertia\" of the model, and fitted sklearn $k$-means models have an `inertia_` attribute.\n",
    "\n",
    "Sometimes you will see the SSE divided by the total sum of squares in the dataset (how far is each point from the center of the entire dataset)\n",
    "\n",
    "Fewer clusters seems better, but inertia will always decrease with _more_ clusters. Hence the idea of looking for an elbow in the plot of inertia vs. $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inertia is the sum of squared distances between points and their cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# From a colleague\n",
    "\n",
    "distortions = []\n",
    "\n",
    "# Calculate SSE for different K\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=301)\n",
    "    kmeans.fit(wine_scaled)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot values of SSE\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_title('Elbow curve')\n",
    "ax.set_xlabel('k')\n",
    "ax.plot(range(2, 10), distortions)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If yellowbrick works for you...\n",
    "# https://www.scikit-yb.org/en/latest/api/cluster/elbow.html\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "\n",
    "visualizer = KElbowVisualizer(model, k=(2,12), timings=True)\n",
    "\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Silhouette Coefficient\n",
    "\n",
    "![silo](images/silo2.png)\n",
    "\n",
    "> **a** refers to the average distance between a point and all other points in that cluster.\n",
    ">\n",
    "> **b** refers to the average distance between that same point and all other points in clusters to which it does not belong\n",
    "\n",
    "It is calculated for each point in the dataset, then averaged across all points for one cumulative score.\n",
    "\n",
    "The Silhouette Coefficient ranges between -1 and 1. The closer to 1, the more clearly defined are the clusters. The closer to -1, the more incorrect assignment.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.silhouette_score(wine_scaled, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = {}\n",
    "\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=301)\n",
    "    kmeans.fit(wine_scaled)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_scores[k] = metrics.silhouette_score(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a colleague\n",
    "from matplotlib import cm\n",
    "\n",
    "km = KMeans(n_clusters=3, random_state=0)\n",
    "y_km = km.fit_predict(wine_scaled)\n",
    "\n",
    "cluster_labels = np.unique(y_km)\n",
    "n_clusters = cluster_labels.shape[0]\n",
    "silhouette_vals = metrics.silhouette_samples(wine_scaled, y_km, metric='euclidean')\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "    c_silhouette_vals.sort()\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, \n",
    "             edgecolor='none', color=color)\n",
    "\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_04.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If yellowbrick works for you...\n",
    "# https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "model = KMeans(3, random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors='yellowbrick')\n",
    "\n",
    "visualizer.fit(wine_scaled)    # Fit the data to the visualizer\n",
    "visualizer.show();             # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So - How many clusters fit the wine data?\n",
    "\n",
    "What can you tell me about them?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering determines cluster assignments by building a hierarchy. This is implemented by either a bottom-up or a top-down approach:\n",
    "\n",
    "- **Agglomerative clustering** is the bottom-up approach. It merges the two points that are the most similar until all points have been merged into a single cluster.\n",
    "- **Divisive clustering** is the top-down approach. It starts with all points as one cluster and splits the least similar clusters at each step until only single data points remain.\n",
    "\n",
    "These methods produce a tree-based hierarchy of points called a **dendrogram**. Similar to partitional clustering, in hierarchical clustering the number of clusters (k) is often predetermined by the user. Clusters are assigned by cutting the dendrogram at a specified depth that results in k groups of smaller dendrograms.\n",
    "\n",
    "![dendro](images/dendogram.png)\n",
    "\n",
    "Unlike many partitional clustering techniques, hierarchical clustering is a deterministic process, meaning cluster assignments won’t change when you run an algorithm twice on the same input data.\n",
    "\n",
    "The **strengths** of hierarchical clustering methods include:\n",
    "\n",
    "- They often reveal the finer details about the relationships between data objects\n",
    "- They provide an interpretable dendrogram\n",
    "\n",
    "The **weaknesses** of hierarchical clustering methods include:\n",
    "\n",
    "- They’re computationally expensive with respect to algorithm complexity\n",
    "- They’re sensitive to noise and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier.plot_agglomerative_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From sklearn's documentation: https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    \"\"\" Create linkage matrix and then plot the dendrogram\"\"\"\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "model = model.fit(wine_scaled)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thoughts?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "## Level Up\n",
    "\n",
    "Using online retail data data from [UCI database](https://archive.ics.uci.edu/ml/datasets/online+retail)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You are looking for patterns so you can get people to buy more, more frequently. \n",
    "You might have to create some new variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping = pd.read_excel('data/Online Retail.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
